#!/usr/bin/env python3
# train_lora_adapter.py - LoRA í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸

import sys
import os
import torch
import pandas as pd
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Union
from dataclasses import dataclass
import logging

# ë¡œì»¬ ëª¨ë“ˆ importë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import (
    LoraConfig, 
    get_peft_model, 
    prepare_model_for_kbit_training,
    TaskType
)
from datasets import Dataset

from app.foundation.config import MODEL_PATH, ADAPTERS_PATH, MODEL_ID
from train_knowledge import KnowledgeTrainer

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class LoRATrainingConfig:
    """LoRA í›ˆë ¨ ì„¤ì •"""
    # LoRA ì„¤ì •
    lora_r: int = 16                    # LoRA rank
    lora_alpha: int = 32                # LoRA alpha
    lora_dropout: float = 0.1           # LoRA dropout
    target_modules: Optional[List[str]] = None    # íƒ€ê²Ÿ ëª¨ë“ˆ
    
    # í›ˆë ¨ ì„¤ì •
    num_epochs: int = 3
    learning_rate: float = 2e-4
    batch_size: int = 2
    gradient_accumulation_steps: int = 4
    max_length: int = 512
    
    # ì €ì¥ ì„¤ì •
    adapter_name: str = "gri_adapter"
    save_steps: int = 100
    eval_steps: int = 50

class LoRATrainer:
    """LoRA ì–´ëŒ‘í„° í›ˆë ¨ í´ë˜ìŠ¤"""
    
    def __init__(self, config: LoRATrainingConfig):
        self.config = config
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = None
        self.tokenizer = None
        self.knowledge_trainer = KnowledgeTrainer()
        
        # ê¸°ë³¸ íƒ€ê²Ÿ ëª¨ë“ˆ ì„¤ì • (GPT-NeoX ì•„í‚¤í…ì²˜ìš©)
        if self.config.target_modules is None:
            self.config.target_modules = [
                "query_key_value",  # attentionì˜ Q, K, V í†µí•© ëª¨ë“ˆ
                "dense",           # attention output projection
                "dense_h_to_4h",   # MLPì˜ ì²« ë²ˆì§¸ linear
                "dense_4h_to_h"    # MLPì˜ ë‘ ë²ˆì§¸ linear
            ]
        
        logger.info(f"LoRA í›ˆë ¨ ì´ˆê¸°í™” - Device: {self.device}")

    def load_base_model(self):
        """ë² ì´ìŠ¤ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©"""
        logger.info(f"ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”©: {MODEL_ID}")
        
        # í† í¬ë‚˜ì´ì € ë¡œë”©
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # ëª¨ë¸ ë¡œë”© (meta tensor ë¬¸ì œ í•´ê²°)
        self.model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.float16,
            device_map=None,  # auto ëŒ€ì‹  None ì‚¬ìš©
            low_cpu_mem_usage=True,
            trust_remote_code=True,
        )
        
        # GPUë¡œ ëª…ì‹œì  ì´ë™
        if torch.cuda.is_available():
            self.model = self.model.to("cuda")
        
        logger.info("âœ… ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

    def prepare_training_data(self) -> Dataset:
        """í›ˆë ¨ ë°ì´í„° ì¤€ë¹„"""
        logger.info("í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
        
        try:
            # GRI ë°ì´í„° ë¡œë”©
            df = self.knowledge_trainer.load_gri_data()
            
            # í›ˆë ¨ ì˜ˆì œ ìƒì„±
            training_examples = []
            
            for _, row in df.iterrows():
                # pandas Seriesì˜ ê°’ì„ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
                try:
                    gongsi = str(row['ê³µì‹œì‚¬í•­']) if pd.notna(row['ê³µì‹œì‚¬í•­']) else ""  # type: ignore
                except (KeyError, ValueError):
                    gongsi = ""
                
                try:
                    yogusahang = str(row['ìš”êµ¬ì‚¬í•­']) if pd.notna(row['ìš”êµ¬ì‚¬í•­']) else ""  # type: ignore
                except (KeyError, ValueError):
                    yogusahang = ""
                
                if gongsi and yogusahang:
                    # ëŒ€í™”í˜• í”„ë¡¬í”„íŠ¸ ìƒì„±
                    prompt = f"### ì§ˆë¬¸: {gongsi}ì— ëŒ€í•œ GRI ìš”êµ¬ì‚¬í•­ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.\n\n### ë‹µë³€: {yogusahang}"
                    training_examples.append({"text": prompt})
            
            # ì¶”ê°€ ESG ê´€ë ¨ ì§ˆë¬¸ë“¤
            esg_examples = [
                {
                    "text": "### ì§ˆë¬¸: ESGê°€ ë¬´ì—‡ì¸ê°€ìš”?\n\n### ë‹µë³€: ESGëŠ” Environmental(í™˜ê²½), Social(ì‚¬íšŒ), Governance(ì§€ë°°êµ¬ì¡°)ì˜ ì¤„ì„ë§ë¡œ, ê¸°ì—…ì˜ ë¹„ì¬ë¬´ì  ì„±ê³¼ë¥¼ ì¸¡ì •í•˜ëŠ” ì§€í‘œì…ë‹ˆë‹¤. ê¸°ì—…ì´ í™˜ê²½ ë³´í˜¸, ì‚¬íšŒì  ì±…ì„, ì§€ë°°êµ¬ì¡° ê°œì„ ì— ì–¼ë§ˆë‚˜ ê¸°ì—¬í•˜ëŠ”ì§€ë¥¼ í‰ê°€í•©ë‹ˆë‹¤."
                },
                {
                    "text": "### ì§ˆë¬¸: GRI ìŠ¤íƒ ë‹¤ë“œë€ ë¬´ì—‡ì¸ê°€ìš”?\n\n### ë‹µë³€: GRI(Global Reporting Initiative) ìŠ¤íƒ ë‹¤ë“œëŠ” ì¡°ì§ì´ ê²½ì œ, í™˜ê²½, ì‚¬íšŒì  ì˜í–¥ì„ ë³´ê³ í•˜ê¸° ìœ„í•œ êµ­ì œì ìœ¼ë¡œ ì¸ì •ë°›ëŠ” ì§€ì†ê°€ëŠ¥ì„± ë³´ê³  í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤."
                },
                {
                    "text": "### ì§ˆë¬¸: ì§€ì†ê°€ëŠ¥ì„± ë³´ê³ ì„œê°€ ì™œ ì¤‘ìš”í•œê°€ìš”?\n\n### ë‹µë³€: ì§€ì†ê°€ëŠ¥ì„± ë³´ê³ ì„œëŠ” ê¸°ì—…ì˜ ESG ì„±ê³¼ë¥¼ íˆ¬ëª…í•˜ê²Œ ê³µê°œí•˜ì—¬ ì´í•´ê´€ê³„ìë“¤ì—ê²Œ ì‹ ë¢°ë¥¼ ì œê³µí•˜ê³ , ê¸°ì—…ì˜ ì¥ê¸°ì  ê°€ì¹˜ ì°½ì¶œì„ ì…ì¦í•˜ëŠ” ì¤‘ìš”í•œ ë„êµ¬ì…ë‹ˆë‹¤."
                }
            ]
            
            training_examples.extend(esg_examples)
            
            logger.info(f"ì´ {len(training_examples)}ê°œì˜ í›ˆë ¨ ì˜ˆì œ ìƒì„±")
            
            # Dataset ê°ì²´ ìƒì„±
            dataset = Dataset.from_list(training_examples)
            
            return dataset
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            raise

    def tokenize_function(self, examples):
        """í† í¬ë‚˜ì´ì§• í•¨ìˆ˜"""
        if self.tokenizer is None:
            raise ValueError("Tokenizerê°€ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        # í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
        tokenized = self.tokenizer(
            examples["text"],
            truncation=True,
            padding=True,
            max_length=self.config.max_length,
            return_tensors="pt"
        )
        
        # GPT-NeoXëŠ” token_type_idsë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì œê±°
        if "token_type_ids" in tokenized:
            del tokenized["token_type_ids"]
        
        # labelsë¥¼ input_idsì™€ ë™ì¼í•˜ê²Œ ì„¤ì • (ì–¸ì–´ ëª¨ë¸ë§)
        tokenized["labels"] = tokenized["input_ids"].clone()
        
        return tokenized

    def setup_lora(self):
        """LoRA ì„¤ì • ë° ëª¨ë¸ ì¤€ë¹„"""
        logger.info("LoRA ì„¤ì • ì¤‘...")
        
        if self.model is None:
            raise ValueError("ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # LoRA ì„¤ì •
        lora_config = LoraConfig(
            r=self.config.lora_r,
            lora_alpha=self.config.lora_alpha,
            target_modules=self.config.target_modules,
            lora_dropout=self.config.lora_dropout,
            bias="none",
            task_type=TaskType.CAUSAL_LM,
        )
        
        # ëª¨ë¸ì— LoRA ì ìš©
        self.model = get_peft_model(self.model, lora_config)  # type: ignore
        
        # í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ì¶œë ¥
        self.model.print_trainable_parameters()
        
        logger.info("âœ… LoRA ì„¤ì • ì™„ë£Œ")

    def train(self):
        """ì‹¤ì œ í›ˆë ¨ ìˆ˜í–‰"""
        logger.info("LoRA í›ˆë ¨ ì‹œì‘...")
        
        try:
            # 1. ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”©
            self.load_base_model()
            
            # 2. LoRA ì„¤ì •
            self.setup_lora()
            
            # 3. í›ˆë ¨ ë°ì´í„° ì¤€ë¹„
            dataset = self.prepare_training_data()
            tokenized_dataset = dataset.map(
                self.tokenize_function,
                batched=True,
                remove_columns=dataset.column_names
            )
            
            # 4. ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
            output_dir = ADAPTERS_PATH / self.config.adapter_name
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # 5. í›ˆë ¨ ì¸ì ì„¤ì •
            training_args = TrainingArguments(
                output_dir=str(output_dir),
                num_train_epochs=self.config.num_epochs,
                per_device_train_batch_size=self.config.batch_size,
                gradient_accumulation_steps=self.config.gradient_accumulation_steps,
                learning_rate=self.config.learning_rate,
                logging_steps=10,
                save_steps=self.config.save_steps,
                eval_steps=self.config.eval_steps,
                warmup_steps=100,
                lr_scheduler_type="cosine",
                fp16=True,
                report_to=None,  # wandb ë“± ì‚¬ìš© ì•ˆí•¨
                remove_unused_columns=False,
            )
            
            # 6. ë°ì´í„° ì½œë ˆì´í„°
            if self.tokenizer is None:
                raise ValueError("Tokenizerê°€ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                
            data_collator = DataCollatorForLanguageModeling(
                tokenizer=self.tokenizer,
                mlm=False,  # ì¸ê³¼ ì–¸ì–´ ëª¨ë¸ë§
            )
            
            # 7. Trainer ìƒì„±
            trainer = Trainer(
                model=self.model,  # type: ignore
                args=training_args,
                train_dataset=tokenized_dataset,
                data_collator=data_collator,
            )
            
            # 8. í›ˆë ¨ ì‹¤í–‰
            logger.info("ğŸš€ í›ˆë ¨ ì‹œì‘!")
            train_result = trainer.train()
            
            # 9. ì–´ëŒ‘í„° ì €ì¥
            trainer.save_model()
            
            # 10. í›ˆë ¨ ë¡œê·¸ ì €ì¥
            log_data = {
                "timestamp": datetime.now().isoformat(),
                "config": self.config.__dict__,
                "train_result": train_result.metrics,
                "adapter_path": str(output_dir)
            }
            
            log_file = output_dir / "training_log.json"
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump(log_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… í›ˆë ¨ ì™„ë£Œ! ì–´ëŒ‘í„° ì €ì¥ ìœ„ì¹˜: {output_dir}")
            logger.info(f"ğŸ“Š í›ˆë ¨ ë¡œê·¸: {log_file}")
            
            return output_dir
            
        except Exception as e:
            logger.error(f"âŒ í›ˆë ¨ ì‹¤íŒ¨: {e}")
            raise

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ LoRA ì–´ëŒ‘í„° í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤!")
    print("="*60)
    
    # í›ˆë ¨ ì„¤ì •
    config = LoRATrainingConfig(
        adapter_name="gri_adapter",
        num_epochs=3,
        batch_size=1,  # GPU ë©”ëª¨ë¦¬ì— ë§ê²Œ ì¡°ì •
        learning_rate=2e-4,
    )
    
    # ì„¤ì • ì¶œë ¥
    print("í›ˆë ¨ ì„¤ì •:")
    print(f"  - LoRA Rank: {config.lora_r}")
    print(f"  - LoRA Alpha: {config.lora_alpha}")
    print(f"  - ì—í¬í¬: {config.num_epochs}")
    print(f"  - ë°°ì¹˜ í¬ê¸°: {config.batch_size}")
    print(f"  - í•™ìŠµë¥ : {config.learning_rate}")
    print(f"  - ì–´ëŒ‘í„° ì´ë¦„: {config.adapter_name}")
    print("="*60)
    
    # ì‚¬ìš©ì í™•ì¸
    response = input("í›ˆë ¨ì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
    if response not in ['y', 'yes']:
        print("í›ˆë ¨ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return
    
    try:
        # í›ˆë ¨ ì‹œì‘
        trainer = LoRATrainer(config)
        adapter_path = trainer.train()
        
        print("\nğŸ‰ í›ˆë ¨ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ğŸ“ ì–´ëŒ‘í„° ìœ„ì¹˜: {adapter_path}")
        print("\në‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì–´ëŒ‘í„°ë¥¼ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
        print("python tests/test_model_loader.py")
        
    except Exception as e:
        print(f"\nâŒ í›ˆë ¨ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 