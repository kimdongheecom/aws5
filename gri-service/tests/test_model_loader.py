#!/usr/bin/env python3
# test_model_loader.py

import sys
import os
import traceback
import json
import time
from datetime import datetime
# tests í´ë”ì—ì„œ ìƒìœ„ í´ë”(gri-service)ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.domain.service.model_loader_service import model_loader_service

def load_previous_feedback(question):
    """ì´ì „ í”¼ë“œë°±ì„ ë¶ˆëŸ¬ì™€ì„œ ì»¨í…ìŠ¤íŠ¸ë¡œ í™œìš©í•©ë‹ˆë‹¤."""
    log_file = "feedback_log.json"
    if not os.path.exists(log_file):
        return ""
    
    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            logs = json.load(f)
    except:
        return ""
    
    # ìœ ì‚¬í•œ ì§ˆë¬¸ì˜ í”¼ë“œë°± ì°¾ê¸°
    relevant_feedback = []
    question_lower = question.lower()
    
    for log in logs:
        if log.get("feedback_type") == "í‹€ë¦°_ë‹µë³€" and log.get("correct_answer"):
            log_question = log.get("question", "").lower()
            # ESG ê´€ë ¨ ì§ˆë¬¸ë“¤ ë§¤ì¹­
            if any(keyword in question_lower and keyword in log_question 
                   for keyword in ["esg", "ê²½ì˜", "í™˜ê²½", "ì‚¬íšŒ", "ì§€ë°°êµ¬ì¡°"]):
                relevant_feedback.append(log)
    
    if not relevant_feedback:
        return ""
    
    # í”¼ë“œë°± ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    feedback_context = "\n[ì´ì „ í”¼ë“œë°± ì°¸ê³ ì‚¬í•­]\n"
    for feedback in relevant_feedback[-3:]:  # ìµœê·¼ 3ê°œë§Œ
        feedback_context += f"ì§ˆë¬¸: {feedback['question']}\n"
        feedback_context += f"í‹€ë¦°ë‹µë³€: {feedback['ai_answer'][:100]}...\n"
        feedback_context += f"ì˜¬ë°”ë¥¸ë‹µë³€: {feedback['correct_answer']}\n\n"
    
    return feedback_context

def save_feedback_log(question, ai_answer, feedback_type, correct_answer=None, generation_time=None):
    """í”¼ë“œë°±ì„ JSON íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
    log_entry = {
        "timestamp": datetime.now().isoformat(),
        "question": question,
        "ai_answer": ai_answer,
        "feedback_type": feedback_type,
        "correct_answer": correct_answer,
        "generation_time_seconds": generation_time
    }
    
    log_file = "feedback_log.json"
    
    # ê¸°ì¡´ ë¡œê·¸ íŒŒì¼ ì½ê¸°
    if os.path.exists(log_file):
        try:
            with open(log_file, 'r', encoding='utf-8') as f:
                logs = json.load(f)
        except:
            logs = []
    else:
        logs = []
    
    # ìƒˆ ë¡œê·¸ ì¶”ê°€
    logs.append(log_entry)
    
    # íŒŒì¼ì— ì €ì¥
    with open(log_file, 'w', encoding='utf-8') as f:
        json.dump(logs, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“„ í”¼ë“œë°±ì´ {log_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def test_model_loader():
    print("ğŸš€ ModelLoaderService í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    try:
        # ëª¨ë¸ ë¡œë“œ
        print("ğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘...")
        model_loader_service.load_model()
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        
        # ëŒ€í™”í˜• ì§ˆë¬¸ ë£¨í”„
        print("\n" + "="*50)
        print("ğŸ’¬ AIì™€ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤!")
        print("ì¢…ë£Œí•˜ë ¤ë©´ 'quit', 'exit', 'ì¢…ë£Œ' ì¤‘ í•˜ë‚˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        print("="*50)
        
        while True:
            try:
                # ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥ë°›ê¸°
                user_question = input("\nğŸ¤” ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
                
                # ì¢…ë£Œ ëª…ë ¹ì–´ í™•ì¸
                if user_question.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:
                    print("ğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤. ì•ˆë…•íˆ ê°€ì„¸ìš”!")
                    break
                
                # ë¹ˆ ì…ë ¥ ì²˜ë¦¬
                if not user_question:
                    print("âš ï¸ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    continue
                
                print(f"\nğŸ¤– AIê°€ ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...")
                
                # ì´ì „ í”¼ë“œë°± ë¶ˆëŸ¬ì˜¤ê¸°
                feedback_context = load_previous_feedback(user_question)
                
                # í”¼ë“œë°±ì„ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
                if feedback_context:
                    enhanced_question = f"{feedback_context}\n\ní˜„ì¬ ì§ˆë¬¸: {user_question}\nìœ„ì˜ í”¼ë“œë°±ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•œ ë‹µë³€ì„ í•´ì£¼ì„¸ìš”."
                    print("ğŸ“š ì´ì „ í”¼ë“œë°±ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤...")
                else:
                    enhanced_question = user_question
                
                # ë‹µë³€ ìƒì„± (ê¸°ë³¸ ì„¤ì •) - ì‹œê°„ ì¸¡ì •
                print("â±ï¸ ë‹µë³€ ìƒì„± ì‹œì‘...")
                start_time = time.time()
                response = model_loader_service.generate(enhanced_question, max_new_tokens=200)
                end_time = time.time()
                generation_time = end_time - start_time
                
                print(f"\nğŸ’¡ AI ë‹µë³€")
                print(f"â±ï¸ ìƒì„±ì‹œê°„: {generation_time:.2f}ì´ˆ")
                print("=" * 60)
                print(response)
                print("=" * 60)
                
                # ë‹µë³€ í‰ê°€ ë° ìˆ˜ì • ì˜µì…˜
                while True:
                    feedback = input("\nğŸ“ ì´ ë‹µë³€ì— ëŒ€í•œ í‰ê°€ë¥¼ í•´ì£¼ì„¸ìš”:\n"
                                   "  1ï¸âƒ£ ì¢‹ì€ ë‹µë³€ (ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ)\n"
                                   "  2ï¸âƒ£ ë‚˜ìœ ë‹µë³€ - ë‹¤ì‹œ ìƒì„±\n"
                                   "  3ï¸âƒ£ ë” ê¸´ ë‹µë³€ìœ¼ë¡œ ì¬ìƒì„±\n"
                                   "  4ï¸âƒ£ ë” ì§§ì€ ë‹µë³€ìœ¼ë¡œ ì¬ìƒì„±\n"
                                   "  5ï¸âƒ£ ì˜¬ë°”ë¥¸ ë‹µë³€ ì§ì ‘ ì…ë ¥\n"
                                   "ì„ íƒ (1-5): ").strip()
                    
                    if feedback == "1":
                        print("âœ… í”¼ë“œë°± ê°ì‚¬í•©ë‹ˆë‹¤!")
                        save_feedback_log(user_question, response, "ì¢‹ì€_ë‹µë³€", generation_time=generation_time)
                        break
                    elif feedback == "2":
                        print("\nğŸ”„ ë‹µë³€ì„ ë‹¤ì‹œ ìƒì„±í•©ë‹ˆë‹¤...")
                        save_feedback_log(user_question, response, "ë‚˜ìœ_ë‹µë³€_ì¬ìƒì„±", generation_time=generation_time)
                        
                        # í”¼ë“œë°± í™œìš©í•œ ì¬ìƒì„±
                        if feedback_context:
                            enhanced_question = f"{feedback_context}\n\ní˜„ì¬ ì§ˆë¬¸: {user_question}\nìœ„ì˜ í”¼ë“œë°±ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•œ ë‹µë³€ì„ í•´ì£¼ì„¸ìš”."
                            print("ğŸ“š ì´ì „ í”¼ë“œë°±ì„ ì°¸ê³ í•˜ì—¬ ì¬ìƒì„±í•©ë‹ˆë‹¤...")
                        else:
                            enhanced_question = user_question
                        
                        print("â±ï¸ ë‹µë³€ ìƒì„± ì‹œì‘...")
                        start_time = time.time()
                        response = model_loader_service.generate(enhanced_question, max_new_tokens=200)
                        end_time = time.time()
                        new_generation_time = end_time - start_time
                        
                        print(f"\nğŸ”„ ì¬ìƒì„±ëœ ë‹µë³€")
                        print(f"â±ï¸ ìƒì„±ì‹œê°„: {new_generation_time:.2f}ì´ˆ")
                        print("=" * 60)
                        print(response)
                        print("=" * 60)
                    elif feedback == "3":
                        print("\nğŸ“ ë” ìì„¸í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤...")
                        save_feedback_log(user_question, response, "ë„ˆë¬´_ì§§ìŒ_ê¸´ë‹µë³€ìš”ì²­", generation_time=generation_time)
                        
                        # í”¼ë“œë°± í™œìš©í•œ ê¸´ ë‹µë³€ ìƒì„±
                        if feedback_context:
                            enhanced_question = f"{feedback_context}\n\ní˜„ì¬ ì§ˆë¬¸: {user_question}\nìœ„ì˜ í”¼ë“œë°±ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ìì„¸í•œ ë‹µë³€ì„ í•´ì£¼ì„¸ìš”."
                            print("ğŸ“š ì´ì „ í”¼ë“œë°±ì„ ì°¸ê³ í•˜ì—¬ ìì„¸í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤...")
                        else:
                            enhanced_question = user_question
                        
                        print("â±ï¸ ê¸´ ë‹µë³€ ìƒì„± ì‹œì‘...")
                        start_time = time.time()
                        response = model_loader_service.generate(enhanced_question, max_new_tokens=400)
                        end_time = time.time()
                        new_generation_time = end_time - start_time
                        
                        print(f"\nğŸ“ ê¸´ ë‹µë³€")
                        print(f"â±ï¸ ìƒì„±ì‹œê°„: {new_generation_time:.2f}ì´ˆ")
                        print("=" * 60)
                        print(response)
                        print("=" * 60)
                    elif feedback == "4":
                        print("\nâš¡ ë” ê°„ë‹¨í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤...")
                        save_feedback_log(user_question, response, "ë„ˆë¬´_ê¹€_ì§§ì€ë‹µë³€ìš”ì²­", generation_time=generation_time)
                        
                        # í”¼ë“œë°± í™œìš©í•œ ì§§ì€ ë‹µë³€ ìƒì„±
                        if feedback_context:
                            enhanced_question = f"{feedback_context}\n\ní˜„ì¬ ì§ˆë¬¸: {user_question}\nìœ„ì˜ í”¼ë“œë°±ì„ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ê°„ë‹¨í•œ ë‹µë³€ì„ í•´ì£¼ì„¸ìš”."
                            print("ğŸ“š ì´ì „ í”¼ë“œë°±ì„ ì°¸ê³ í•˜ì—¬ ê°„ë‹¨í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤...")
                        else:
                            enhanced_question = user_question
                        
                        print("â±ï¸ ì§§ì€ ë‹µë³€ ìƒì„± ì‹œì‘...")
                        start_time = time.time()
                        response = model_loader_service.generate(enhanced_question, max_new_tokens=100)
                        end_time = time.time()
                        new_generation_time = end_time - start_time
                        
                        print(f"\nâš¡ ì§§ì€ ë‹µë³€")
                        print(f"â±ï¸ ìƒì„±ì‹œê°„: {new_generation_time:.2f}ì´ˆ")
                        print("=" * 60)
                        print(response)
                        print("=" * 60)
                    elif feedback == "5":
                        correct_answer = input("\nâœï¸ ì˜¬ë°”ë¥¸ ë‹µë³€ì„ ì…ë ¥í•´ì£¼ì„¸ìš”: ").strip()
                        if correct_answer:
                            print(f"\nâœ… ì˜¬ë°”ë¥¸ ë‹µë³€ì´ ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤:")
                            print("-" * 40)
                            print(correct_answer)
                            print("-" * 40)
                            print("ğŸ“š í–¥í›„ ëª¨ë¸ ê°œì„ ì— í™œìš©í•˜ê² ìŠµë‹ˆë‹¤.")
                            save_feedback_log(user_question, response, "í‹€ë¦°_ë‹µë³€", correct_answer, generation_time)
                        break
                    else:
                        print("âš ï¸ 1-5 ì¤‘ì—ì„œ ì„ íƒí•´ì£¼ì„¸ìš”.")
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ Ctrl+Cë¡œ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                traceback.print_exc()
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    test_model_loader() 