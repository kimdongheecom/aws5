# gri-service/app/domain/service/model_loader_service.py

import torch
import logging
import os
import traceback
from typing import Union, Optional, Any, Dict
from transformers import AutoTokenizer, AutoModelForCausalLM
from huggingface_hub import snapshot_download

# PEFT ê´€ë ¨ import (Optional ì²˜ë¦¬)
from peft import PeftModel



from app.foundation.config import MODEL_ID, MODEL_PATH, ADAPTERS_PATH, DEFAULT_ADAPTER, GRI_ADAPTER_CONFIG

# RTX 5060 (Ada Lovelace) í˜¸í™˜ì„± ì„¤ì •
os.environ["TORCH_CUDA_ARCH_LIST"] = "9.0"
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
os.environ["TORCH_ALLOW_TF32_CUBLAS_OVERRIDE"] = "1"


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModelLoaderService:
    """AI ëª¨ë¸ ë¡œë”©, ë‹¤ìš´ë¡œë“œ, ì¶”ë¡ ì„ ë‹´ë‹¹í•˜ëŠ” ì„œë¹„ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)"""
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(ModelLoaderService, cls).__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if self._initialized:
            return
        # íƒ€ì… íŒíŠ¸ ëª…ì‹œ (ì‹¤ìš©ì  ì ‘ê·¼)
        self.model: Optional[Any] = None  # transformers/peft ëª¨ë¸ë“¤ì˜ ë³µì¡í•œ íƒ€ì… êµ¬ì¡°ë¡œ ì¸í•´ Any ì‚¬ìš©
        self.tokenizer: Optional[Any] = None  # AutoTokenizerì˜ ë™ì  ì†ì„±ìœ¼ë¡œ ì¸í•´ Any ì‚¬ìš©
        self.current_adapter: Optional[str] = None
        self.adapters_loaded: Dict[str, bool] = {}  # ë¡œë”©ëœ ì–´ëŒ‘í„°ë“¤ì„ ìºì‹œ
        # RTX 5060ì„ ê°•ì œë¡œ ì‚¬ìš©
        self.device = "cuda"
        self._initialized = True
        logger.info(f"ModelLoaderService ì´ˆê¸°í™” (Device: {self.device})")
        
    def _check_peft_available(self) -> bool:
        """PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸"""
        return True
        
    def _download_model_if_not_exists(self):
        """ì§€ì •ëœ ê²½ë¡œì— ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""
        # config.json íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì—¬ ë‹¤ìš´ë¡œë“œ ì—¬ë¶€ ê²°ì •
        model_config_path = MODEL_PATH / "config.json"
        if not MODEL_PATH.exists() or not list(MODEL_PATH.glob("*.json")): # í´ë” ë‚´ì— json íŒŒì¼ì´ í•˜ë‚˜ë„ ì—†ë‹¤ë©´
            logger.info(f"'{MODEL_PATH}'ì— ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. '{MODEL_ID}' ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            snapshot_download(
                repo_id=MODEL_ID,
                local_dir=MODEL_PATH,
                local_dir_use_symlinks=False,
            )
            logger.info("âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        else:
            logger.info(f"'{MODEL_PATH}'ì—ì„œ ê¸°ì¡´ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    def load_model(self):
        """ì•± ì‹œì‘ ì‹œ ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œí•©ë‹ˆë‹¤."""
        if self.model:
            logger.info("ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return

        self._download_model_if_not_exists()
        
        try:
            logger.info(f"ğŸ§  ëª¨ë¸ ë¡œë”© ì‹œì‘: {MODEL_ID} (GPU ì§ì ‘ ë¡œë”©)")
            
            # Windows bitsandbytes ë¬¸ì œë¡œ ì¸í•´ 4ë¹„íŠ¸ ì–‘ìí™” ë¹„í™œì„±í™”
            # quantization_config = BitsAndBytesConfig(
            #     load_in_4bit=True,
            #     bnb_4bit_quant_type="nf4",
            #     bnb_4bit_compute_dtype=torch.float16,
            #     bnb_4bit_use_double_quant=True,
            # )
            
            self.model = AutoModelForCausalLM.from_pretrained(
                MODEL_PATH, 
                # quantization_config=quantization_config,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                use_safetensors=True,
                # RTX 5060 í˜¸í™˜ì„±ì„ ìœ„í•œ ì¶”ê°€ ì„¤ì •
                attn_implementation="eager",  # Flash Attention ë¹„í™œì„±í™”
            )
            # --- ğŸ’¡ ìˆ˜ì •ëœ ë¶€ë¶„ ë ğŸ’¡ ---
            
            self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
            
            # í† í¬ë‚˜ì´ì € íŒ¨ë”© í† í° ì„¤ì • (ì•ˆì „í•œ ì†ì„± ì ‘ê·¼)
            pad_token = getattr(self.tokenizer, 'pad_token', None)
            if pad_token is None:
                eos_token = getattr(self.tokenizer, 'eos_token', None)
                if eos_token is not None:
                    setattr(self.tokenizer, 'pad_token', eos_token)
                
            logger.info("âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ!")
            
            # ê¸°ë³¸ LoRA ì–´ëŒ‘í„° ë¡œë”© ì‹œë„
            if GRI_ADAPTER_CONFIG["enabled"]:
                self._load_default_adapter()
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}", exc_info=True)
            raise

    def _load_default_adapter(self):
        """ê¸°ë³¸ GRI ì–´ëŒ‘í„°ë¥¼ ë¡œë”©í•©ë‹ˆë‹¤."""
        try:
            adapter_path = ADAPTERS_PATH / DEFAULT_ADAPTER
            if adapter_path.exists() and any(adapter_path.glob("*.safetensors")):
                logger.info(f"ğŸ”§ ê¸°ë³¸ LoRA ì–´ëŒ‘í„° ë¡œë”© ì¤‘: {DEFAULT_ADAPTER}")
                self.load_adapter(DEFAULT_ADAPTER)
                logger.info(f"âœ… ê¸°ë³¸ ì–´ëŒ‘í„° '{DEFAULT_ADAPTER}' ë¡œë”© ì™„ë£Œ!")
            else:
                logger.info(f"âš ï¸ ê¸°ë³¸ ì–´ëŒ‘í„° '{DEFAULT_ADAPTER}'ê°€ ì—†ìŠµë‹ˆë‹¤. ë² ì´ìŠ¤ ëª¨ë¸ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
        except Exception as e:
            logger.warning(f"âš ï¸ ê¸°ë³¸ ì–´ëŒ‘í„° ë¡œë”© ì‹¤íŒ¨, ë² ì´ìŠ¤ ëª¨ë¸ ì‚¬ìš©: {e}")

    def load_adapter(self, adapter_name: str):
        """íŠ¹ì • LoRA ì–´ëŒ‘í„°ë¥¼ ë¡œë”©í•©ë‹ˆë‹¤."""
        if not self._check_peft_available():
            raise RuntimeError("PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
        if not self.model:
            raise RuntimeError("ë² ì´ìŠ¤ ëª¨ë¸ì´ ë¨¼ì € ë¡œë“œë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
        
        adapter_path = ADAPTERS_PATH / adapter_name
        if not adapter_path.exists():
            raise FileNotFoundError(f"ì–´ëŒ‘í„° í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {adapter_path}")
        
        try:
            # ê¸°ì¡´ ì–´ëŒ‘í„°ê°€ ìˆìœ¼ë©´ ì œê±°
            if self.current_adapter:
                logger.info(f"ğŸ”„ ê¸°ì¡´ ì–´ëŒ‘í„° '{self.current_adapter}' ì œê±° ì¤‘...")
                self.unload_adapter()
            
            logger.info(f"ğŸ”§ LoRA ì–´ëŒ‘í„° ë¡œë”© ì¤‘: {adapter_name}")
            
            # PEFT ëª¨ë¸ë¡œ ë³€í™˜ (ì–´ëŒ‘í„° ë¡œë”©) - ì•ˆì „í•œ íƒ€ì… ì²˜ë¦¬
            self.model = PeftModel.from_pretrained(  # type: ignore
                self.model, 
                adapter_path,
                is_trainable=False,
                device_map="auto"
            )
            
            self.current_adapter = adapter_name
            self.adapters_loaded[adapter_name] = True
            
            logger.info(f"âœ… LoRA ì–´ëŒ‘í„° '{adapter_name}' ë¡œë”© ì™„ë£Œ!")
            
        except Exception as e:
            logger.error(f"âŒ ì–´ëŒ‘í„° '{adapter_name}' ë¡œë”© ì‹¤íŒ¨: {e}")
            raise

    def unload_adapter(self):
        """í˜„ì¬ ë¡œë”©ëœ LoRA ì–´ëŒ‘í„°ë¥¼ ì œê±°í•©ë‹ˆë‹¤."""
        if not self.current_adapter:
            logger.info("ì œê±°í•  ì–´ëŒ‘í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            logger.info(f"ğŸ—‘ï¸ ì–´ëŒ‘í„° '{self.current_adapter}' ì œê±° ì¤‘...")
            
            # PEFT ëª¨ë¸ì—ì„œ ë² ì´ìŠ¤ ëª¨ë¸ë¡œ ë˜ëŒë¦¬ê¸° (ì•ˆì „í•œ ì†ì„± ì ‘ê·¼)
            if hasattr(self.model, 'unload') and callable(getattr(self.model, 'unload')):
                self.model.unload()  # type: ignore
            elif hasattr(self.model, 'base_model'):
                self.model = getattr(self.model, 'base_model')  # type: ignore
            
            self.current_adapter = None
            logger.info("âœ… ì–´ëŒ‘í„° ì œê±° ì™„ë£Œ!")
            
        except Exception as e:
            logger.error(f"âŒ ì–´ëŒ‘í„° ì œê±° ì‹¤íŒ¨: {e}")
            raise

    def switch_adapter(self, adapter_name: str):
        """ë‹¤ë¥¸ LoRA ì–´ëŒ‘í„°ë¡œ ì „í™˜í•©ë‹ˆë‹¤."""
        if self.current_adapter == adapter_name:
            logger.info(f"ì´ë¯¸ '{adapter_name}' ì–´ëŒ‘í„°ê°€ ë¡œë”©ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return
        
        logger.info(f"ğŸ”„ ì–´ëŒ‘í„° ì „í™˜: {self.current_adapter} â†’ {adapter_name}")
        self.load_adapter(adapter_name)

    def get_adapter_info(self):
        """í˜„ì¬ ì–´ëŒ‘í„° ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return {
            "current_adapter": self.current_adapter,
            "available_adapters": list(self.adapters_loaded.keys()),
            "adapter_config": GRI_ADAPTER_CONFIG if self.current_adapter == DEFAULT_ADAPTER else None
        }

    def generate(self, prompt: str, max_new_tokens: int = 256) -> str:
        """í”„ë¡¬í”„íŠ¸ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.model or not self.tokenizer:
            raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # í˜„ì¬ ì–´ëŒ‘í„° ì •ë³´ ë¡œê¹…
        adapter_info = f" (ì–´ëŒ‘í„°: {self.current_adapter})" if self.current_adapter else " (ë² ì´ìŠ¤ ëª¨ë¸)"
        logger.info(f"ğŸ¤– í…ìŠ¤íŠ¸ ìƒì„± ì‹œì‘{adapter_info}")
            
        # KoAlpaca ëŒ€í™”í˜• í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ êµ¬ì„±
        conversation_prompt = f"""### ì§ˆë¬¸: {prompt}

### ë‹µë³€:"""
        
        inputs = self.tokenizer(conversation_prompt, return_tensors="pt", add_special_tokens=True)
        # token_type_ids ì œê±° (GPT ê³„ì—´ ëª¨ë¸ì—ì„œëŠ” ë¶ˆí•„ìš”)
        if 'token_type_ids' in inputs:
            del inputs['token_type_ids']
        inputs = inputs.to(self.device)
        
        with torch.no_grad():
            # RTX 5060 í˜¸í™˜ì„±ì„ ìœ„í•œ ì•ˆì „í•œ ìƒì„± ì„¤ì •
            try:
                # ëª¨ë¸ì˜ generate ë©”ì„œë“œ ì•ˆì „í•˜ê²Œ í˜¸ì¶œ
                generate_method = getattr(self.model, 'generate', None)
                if generate_method is None or not callable(generate_method):
                    raise RuntimeError("ëª¨ë¸ì— generate ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
                
                outputs = generate_method(  # type: ignore
                    **inputs, 
                    max_new_tokens=max_new_tokens,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.1,
                    pad_token_id=getattr(self.tokenizer, 'eos_token_id', None),
                    eos_token_id=getattr(self.tokenizer, 'eos_token_id', None),
                    use_cache=True,
                    output_attentions=False,
                    output_hidden_states=False,
                )
            except RuntimeError as e:
                if "no kernel image is available" in str(e):
                    logger.warning("CUDA ì»¤ë„ í˜¸í™˜ì„± ë¬¸ì œ ê°ì§€. ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ ì‹œë„í•©ë‹ˆë‹¤...")
                    # ë” ì•ˆì „í•œ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„
                    outputs = generate_method(  # type: ignore
                        **inputs, 
                        max_new_tokens=min(max_new_tokens, 50),  # í† í° ìˆ˜ ì œí•œ
                        do_sample=False,  # ê·¸ë¦¬ë”” ë””ì½”ë”©
                        pad_token_id=getattr(self.tokenizer, 'eos_token_id', None),
                        eos_token_id=getattr(self.tokenizer, 'eos_token_id', None),
                        use_cache=False,
                    )
                else:
                    raise
        
        response_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)  # type: ignore
        # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ì„ ì œì™¸í•˜ê³  ë‹µë³€ë§Œ ë°˜í™˜
        answer_start = response_text.find("### ë‹µë³€:") + len("### ë‹µë³€:")
        answer = response_text[answer_start:].strip()
        
        # ë‹¤ìŒ ì§ˆë¬¸ì´ ì‹œì‘ë˜ë©´ ê·¸ ì „ê¹Œì§€ë§Œ ë°˜í™˜
        if "### ì§ˆë¬¸:" in answer:
            answer = answer.split("### ì§ˆë¬¸:")[0].strip()
            
        return answer


model_loader_service = ModelLoaderService()

# --- ì•„ë˜ í…ŒìŠ¤íŠ¸ ì½”ë“œë¥¼ íŒŒì¼ ë§¨ ëì— ì¶”ê°€í•˜ì„¸ìš” ---
if __name__ == "__main__":
    try:
        print("="*50)
        print("ëª¨ë¸ ë¡œë” ì„œë¹„ìŠ¤ ë‹¨ë… ì‹¤í–‰ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        print("="*50)
        
        # ìœ„ì—ì„œ ìƒì„±ëœ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ì˜ load_model í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
        model_loader_service.load_model()
        
        print("\nğŸ‰ğŸ‰ğŸ‰ í…ŒìŠ¤íŠ¸ ì„±ê³µ! ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ğŸ‰ğŸ‰ğŸ‰")
        
    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë” ìì„¸í•œ ì •ë³´ë¥¼ ë³¼ ìˆ˜ ìˆë„ë¡ traceback ì¶”ê°€
        print(f"\nğŸ”¥ğŸ”¥ğŸ”¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨! ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e} ğŸ”¥ğŸ”¥ğŸ”¥")
        traceback.print_exc()