# C:\Users\edh48\Documents\aws5\gri-service\app\domain\service\model_loader_service.py

import torch
import logging
import os
import traceback
from typing import Union, Optional, Any, Dict
from transformers import AutoTokenizer, AutoModelForCausalLM
from huggingface_hub import snapshot_download

from peft import PeftModel

from app.foundation.config import MODEL_ID, MODEL_PATH, ADAPTERS_PATH, DEFAULT_ADAPTER, GRI_ADAPTER_CONFIG

# RTX 5060ìš© CUDA ì„¤ì •
os.environ["TORCH_CUDA_ARCH_LIST"] = "8.9+PTX"  # RTX 5060 í˜¸í™˜
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
os.environ["TORCH_ALLOW_TF32_CUBLAS_OVERRIDE"] = "1"
os.environ["TORCH_USE_CUDA_DSA"] = "1"  # CUDA DSA í™œì„±í™”
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"  # ë©”ëª¨ë¦¬ ë¶„í•  ìµœì í™”

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModelLoaderService:
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(ModelLoaderService, cls).__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if self._initialized:
            return
        self.model: Optional[Any] = None
        self.tokenizer: Optional[Any] = None
        self.current_adapter: Optional[str] = None
        self.adapters_loaded: Dict[str, bool] = {}
        
        self.device = "cuda"
        
        self._initialized = True
        logger.info(f"ModelLoaderService ì´ˆê¸°í™” (Device: {self.device})")
        
    def _check_peft_available(self) -> bool:
        return True
        
    def _download_model_if_not_exists(self):
        if not MODEL_PATH.exists() or not list(MODEL_PATH.glob("*.json")):
            logger.info(f"'{MODEL_PATH}'ì— ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. '{MODEL_ID}' ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            snapshot_download(
                repo_id=MODEL_ID,
                local_dir=MODEL_PATH,
                local_dir_use_symlinks=False,
            )
            logger.info("âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        else:
            logger.info(f"'{MODEL_PATH}'ì—ì„œ ê¸°ì¡´ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    def load_model(self):
        if self.model:
            logger.info("ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return

        self._download_model_if_not_exists()
        
        try:
            logger.info(f"ğŸ§  ëª¨ë¸ ë¡œë”© ì‹œì‘: {MODEL_ID} (GPU ì§ì ‘ ë¡œë”©)")
            
            self.model = AutoModelForCausalLM.from_pretrained(
                MODEL_PATH, 
                torch_dtype=torch.float16,
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                use_safetensors=True,
                attn_implementation="eager",
            )
            
            # ëª¨ë¸ì„ GPUë¡œ ëª…ì‹œì ìœ¼ë¡œ ì´ë™
            self.model = self.model.to(self.device)
            
            self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
            
            pad_token = getattr(self.tokenizer, 'pad_token', None)
            if pad_token is None:
                eos_token = getattr(self.tokenizer, 'eos_token', None)
                if eos_token is not None:
                    setattr(self.tokenizer, 'pad_token', eos_token)
                
            logger.info("âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ!")
            
            if GRI_ADAPTER_CONFIG["enabled"]:
                self._load_default_adapter()
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}", exc_info=True)
            raise

    def _load_default_adapter(self):
        try:
            adapter_path = ADAPTERS_PATH / DEFAULT_ADAPTER
            if adapter_path.exists() and any(adapter_path.glob("*.safetensors")):
                logger.info(f"ğŸ”§ ê¸°ë³¸ LoRA ì–´ëŒ‘í„° ë¡œë”© ì¤‘: {DEFAULT_ADAPTER}")
                self.load_adapter(DEFAULT_ADAPTER)
                logger.info(f"âœ… ê¸°ë³¸ ì–´ëŒ‘í„° '{DEFAULT_ADAPTER}' ë¡œë”© ì™„ë£Œ!")
            else:
                logger.info(f"âš ï¸ ê¸°ë³¸ ì–´ëŒ‘í„° '{DEFAULT_ADAPTER}'ê°€ ì—†ìŠµë‹ˆë‹¤. ë² ì´ìŠ¤ ëª¨ë¸ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
        except Exception as e:
            logger.warning(f"âš ï¸ ê¸°ë³¸ ì–´ëŒ‘í„° ë¡œë”© ì‹¤íŒ¨, ë² ì´ìŠ¤ ëª¨ë¸ ì‚¬ìš©: {e}")

    def load_adapter(self, adapter_name: str):
        if not self._check_peft_available():
            raise RuntimeError("PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
        if not self.model:
            raise RuntimeError("ë² ì´ìŠ¤ ëª¨ë¸ì´ ë¨¼ì € ë¡œë“œë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
        
        adapter_path = ADAPTERS_PATH / adapter_name
        if not adapter_path.exists():
            raise FileNotFoundError(f"ì–´ëŒ‘í„° í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {adapter_path}")
        
        try:
            if self.current_adapter:
                logger.info(f"ğŸ”„ ê¸°ì¡´ ì–´ëŒ‘í„° '{self.current_adapter}' ì œê±° ì¤‘...")
                self.unload_adapter()
            
            logger.info(f"ğŸ”§ LoRA ì–´ëŒ‘í„° ë¡œë”© ì¤‘: {adapter_name}")
            
            self.model = PeftModel.from_pretrained(
                self.model, 
                adapter_path,
                is_trainable=False
            )
            
            # ì–´ëŒ‘í„° ë¡œë”© í›„ì—ë„ GPUë¡œ ì´ë™
            self.model = self.model.to(self.device)
            
            self.current_adapter = adapter_name
            self.adapters_loaded[adapter_name] = True
            
            logger.info(f"âœ… LoRA ì–´ëŒ‘í„° '{adapter_name}' ë¡œë”© ì™„ë£Œ!")
            
        except Exception as e:
            logger.error(f"âŒ ì–´ëŒ‘í„° '{adapter_name}' ë¡œë”© ì‹¤íŒ¨: {e}")
            raise

    def unload_adapter(self):
        if not self.current_adapter:
            logger.info("ì œê±°í•  ì–´ëŒ‘í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            logger.info(f"ğŸ—‘ï¸ ì–´ëŒ‘í„° '{self.current_adapter}' ì œê±° ì¤‘...")
            
            if hasattr(self.model, 'unload') and callable(getattr(self.model, 'unload')):
                self.model.unload()
            elif hasattr(self.model, 'base_model'):
                self.model = getattr(self.model, 'base_model')
            
            self.current_adapter = None
            logger.info("âœ… ì–´ëŒ‘í„° ì œê±° ì™„ë£Œ!")
            
        except Exception as e:
            logger.error(f"âŒ ì–´ëŒ‘í„° ì œê±° ì‹¤íŒ¨: {e}")
            raise

    def switch_adapter(self, adapter_name: str):
        if self.current_adapter == adapter_name:
            logger.info(f"ì´ë¯¸ '{adapter_name}' ì–´ëŒ‘í„°ê°€ ë¡œë”©ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return
        
        logger.info(f"ğŸ”„ ì–´ëŒ‘í„° ì „í™˜: {self.current_adapter} â†’ {adapter_name}")
        self.load_adapter(adapter_name)

    def get_adapter_info(self):
        return {
            "current_adapter": self.current_adapter,
            "available_adapters": list(self.adapters_loaded.keys()),
            "adapter_config": GRI_ADAPTER_CONFIG if self.current_adapter == DEFAULT_ADAPTER else None
        }

    def generate(self, prompt: str, max_new_tokens: int = 512) -> str:
        """í”„ë¡¬í”„íŠ¸ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.model or not self.tokenizer:
            raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        adapter_info = f" (ì–´ëŒ‘í„°: {self.current_adapter})" if self.current_adapter else " (ë² ì´ìŠ¤ ëª¨ë¸)"
        logger.info(f"ğŸ¤– í…ìŠ¤íŠ¸ ìƒì„± ì‹œì‘{adapter_info}")
        
        # --- â–¼â–¼â–¼ í•µì‹¬ ìˆ˜ì • ë¶€ë¶„ ì‹œì‘ â–¼â–¼â–¼ ---
        inputs = self.tokenizer(prompt, return_tensors="pt", add_special_tokens=True)
        # 1. ì…ë ¥ í”„ë¡¬í”„íŠ¸ì˜ í† í° ê¸¸ì´ë¥¼ ë¯¸ë¦¬ ì €ì¥í•©ë‹ˆë‹¤.
        input_ids_length = inputs['input_ids'].shape[1]
        # --- â–²â–²â–² í•µì‹¬ ìˆ˜ì • ë¶€ë¶„ ë â–²â–²â–² ---

        if 'token_type_ids' in inputs:
            del inputs['token_type_ids']
        
        # ëª¨ë“  ì…ë ¥ì„ GPUë¡œ ì´ë™
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            try:
                generate_method = getattr(self.model, 'generate', None)
                if generate_method is None or not callable(generate_method):
                    raise RuntimeError("ëª¨ë¸ì— generate ë©”ì„œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
                
                outputs = generate_method(
                    **inputs, 
                    max_new_tokens=max_new_tokens,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.1,
                    pad_token_id=getattr(self.tokenizer, 'eos_token_id', None),
                    eos_token_id=getattr(self.tokenizer, 'eos_token_id', None),
                    use_cache=True,
                    output_attentions=False,
                    output_hidden_states=False,
                )
            except RuntimeError as e:
                if "no kernel image is available" in str(e):
                    logger.warning("CUDA ì»¤ë„ í˜¸í™˜ì„± ë¬¸ì œ ê°ì§€. ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ ì‹œë„í•©ë‹ˆë‹¤...")
                    outputs = generate_method(
                        **inputs, 
                        max_new_tokens=min(max_new_tokens, 50),
                        do_sample=False,
                        pad_token_id=getattr(self.tokenizer, 'eos_token_id', None),
                        eos_token_id=getattr(self.tokenizer, 'eos_token_id', None),
                        use_cache=False,
                    )
                else:
                    raise
        
        # --- â–¼â–¼â–¼ í•µì‹¬ ìˆ˜ì • ë¶€ë¶„ ì‹œì‘ â–¼â–¼â–¼ ---
        # 2. ì „ì²´ ê²°ê³¼ì—ì„œ ì…ë ¥ í† í° ê¸¸ì´ë§Œí¼ì„ ì˜ë¼ë‚´ì–´, ìƒˆë¡œ ìƒì„±ëœ í† í°ë§Œ ë¶„ë¦¬í•©ë‹ˆë‹¤.
        generated_token_ids = outputs[0][input_ids_length:]
        # 3. ë¶„ë¦¬ëœ í† í°ë“¤ë§Œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        answer = self.tokenizer.decode(generated_token_ids, skip_special_tokens=True).strip()
        # --- â–²â–²â–² í•µì‹¬ ìˆ˜ì • ë¶€ë¶„ ë â–²â–²â–² ---
        
        logger.info(f"ğŸ¤–ğŸ¤–ğŸ¤–ğŸ¤–ğŸ¤– GRI ë‹µë³€ ìƒì„± ì™„ë£Œ: {answer}")
        
        # íŠ¹ì • ë¬¸ìì—´ì— ì˜ì¡´í•˜ëŠ” ë¶ˆì•ˆì •í•œ í›„ì²˜ë¦¬ ë¡œì§ì€ ì œê±°í•˜ê±°ë‚˜ ì‹ ì¤‘í•˜ê²Œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
        if "### ì§ˆë¬¸:" in answer:
            answer = answer.split("### ì§ˆë¬¸:")[0].strip()
            
        return answer

model_loader_service = ModelLoaderService()

if __name__ == "__main__":
    try:
        print("="*50)
        print("ëª¨ë¸ ë¡œë” ì„œë¹„ìŠ¤ ë‹¨ë… ì‹¤í–‰ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        print("="*50)
        
        model_loader_service.load_model()
        
        print("\nğŸ‰ğŸ‰ğŸ‰ í…ŒìŠ¤íŠ¸ ì„±ê³µ! ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ğŸ‰ğŸ‰ğŸ‰")
        
    except Exception as e:
        print(f"\nğŸ”¥ğŸ”¥ğŸ”¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨! ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e} ğŸ”¥ğŸ”¥ğŸ”¥")
        traceback.print_exc()