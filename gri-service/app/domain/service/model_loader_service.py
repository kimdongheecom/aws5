# gri-service/app/domain/service/model_loader_service.py

import torch
import logging
import os
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers.utils.quantization_config import BitsAndBytesConfig
from huggingface_hub import snapshot_download
from app.foundation.config import MODEL_ID, MODEL_PATH

# RTX 5060 (Ada Lovelace) í˜¸í™˜ì„± ì„¤ì •
os.environ["TORCH_CUDA_ARCH_LIST"] = "9.0"
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
os.environ["TORCH_ALLOW_TF32_CUBLAS_OVERRIDE"] = "1"


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModelLoaderService:
    """AI ëª¨ë¸ ë¡œë”©, ë‹¤ìš´ë¡œë“œ, ì¶”ë¡ ì„ ë‹´ë‹¹í•˜ëŠ” ì„œë¹„ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)"""
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(ModelLoaderService, cls).__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self):
        if self._initialized:
            return
        self.model = None
        self.tokenizer = None
        # RTX 5060ì„ ê°•ì œë¡œ ì‚¬ìš©
        self.device = "cuda"
        self._initialized = True
        logger.info(f"ModelLoaderService ì´ˆê¸°í™” (Device: {self.device})")
        
    def _download_model_if_not_exists(self):
        """ì§€ì •ëœ ê²½ë¡œì— ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""
        # config.json íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì—¬ ë‹¤ìš´ë¡œë“œ ì—¬ë¶€ ê²°ì •
        model_config_path = MODEL_PATH / "config.json"
        if not MODEL_PATH.exists() or not list(MODEL_PATH.glob("*.json")): # í´ë” ë‚´ì— json íŒŒì¼ì´ í•˜ë‚˜ë„ ì—†ë‹¤ë©´
            logger.info(f"'{MODEL_PATH}'ì— ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. '{MODEL_ID}' ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            snapshot_download(
                repo_id=MODEL_ID,
                local_dir=MODEL_PATH,
                local_dir_use_symlinks=False,
            )
            logger.info("âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        else:
            logger.info(f"'{MODEL_PATH}'ì—ì„œ ê¸°ì¡´ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    def load_model(self):
        """ì•± ì‹œì‘ ì‹œ ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œí•©ë‹ˆë‹¤."""
        if self.model:
            logger.info("ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return

        self._download_model_if_not_exists()
        
        try:
            logger.info(f"ğŸ§  ëª¨ë¸ ë¡œë”© ì‹œì‘: {MODEL_ID} (GPU ì§ì ‘ ë¡œë”©)")
            
            # Windows bitsandbytes ë¬¸ì œë¡œ ì¸í•´ 4ë¹„íŠ¸ ì–‘ìí™” ë¹„í™œì„±í™”
            # quantization_config = BitsAndBytesConfig(
            #     load_in_4bit=True,
            #     bnb_4bit_quant_type="nf4",
            #     bnb_4bit_compute_dtype=torch.float16,
            #     bnb_4bit_use_double_quant=True,
            # )
            
            self.model = AutoModelForCausalLM.from_pretrained(
                MODEL_PATH, 
                # quantization_config=quantization_config,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                use_safetensors=True,
                # RTX 5060 í˜¸í™˜ì„±ì„ ìœ„í•œ ì¶”ê°€ ì„¤ì •
                attn_implementation="eager",  # Flash Attention ë¹„í™œì„±í™”
            )
            # --- ğŸ’¡ ìˆ˜ì •ëœ ë¶€ë¶„ ë ğŸ’¡ ---
            
            self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                
            logger.info("âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ!")
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}", exc_info=True)
            raise

    def generate(self, prompt: str, max_new_tokens: int = 256) -> str:
        """í”„ë¡¬í”„íŠ¸ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.model or not self.tokenizer:
            raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        # KoAlpaca ëŒ€í™”í˜• í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ êµ¬ì„±
        conversation_prompt = f"""### ì§ˆë¬¸: {prompt}

### ë‹µë³€:"""
        
        inputs = self.tokenizer(conversation_prompt, return_tensors="pt", add_special_tokens=True)
        # token_type_ids ì œê±° (GPT ê³„ì—´ ëª¨ë¸ì—ì„œëŠ” ë¶ˆí•„ìš”)
        if 'token_type_ids' in inputs:
            del inputs['token_type_ids']
        inputs = inputs.to(self.device)
        
        with torch.no_grad():
            # RTX 5060 í˜¸í™˜ì„±ì„ ìœ„í•œ ì•ˆì „í•œ ìƒì„± ì„¤ì •
            try:
                outputs = self.model.generate(
                    **inputs, 
                    max_new_tokens=max_new_tokens,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.1,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    use_cache=True,
                    output_attentions=False,
                    output_hidden_states=False,
                )
            except RuntimeError as e:
                if "no kernel image is available" in str(e):
                    logger.warning("CUDA ì»¤ë„ í˜¸í™˜ì„± ë¬¸ì œ ê°ì§€. ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ ì‹œë„í•©ë‹ˆë‹¤...")
                    # ë” ì•ˆì „í•œ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„
                    outputs = self.model.generate(
                        **inputs, 
                        max_new_tokens=min(max_new_tokens, 50),  # í† í° ìˆ˜ ì œí•œ
                        do_sample=False,  # ê·¸ë¦¬ë”” ë””ì½”ë”©
                        pad_token_id=self.tokenizer.eos_token_id,
                        eos_token_id=self.tokenizer.eos_token_id,
                        use_cache=False,
                    )
                else:
                    raise
        
        response_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ì„ ì œì™¸í•˜ê³  ë‹µë³€ë§Œ ë°˜í™˜
        answer_start = response_text.find("### ë‹µë³€:") + len("### ë‹µë³€:")
        answer = response_text[answer_start:].strip()
        
        # ë‹¤ìŒ ì§ˆë¬¸ì´ ì‹œì‘ë˜ë©´ ê·¸ ì „ê¹Œì§€ë§Œ ë°˜í™˜
        if "### ì§ˆë¬¸:" in answer:
            answer = answer.split("### ì§ˆë¬¸:")[0].strip()
            
        return answer


model_loader_service = ModelLoaderService()

# --- ì•„ë˜ í…ŒìŠ¤íŠ¸ ì½”ë“œë¥¼ íŒŒì¼ ë§¨ ëì— ì¶”ê°€í•˜ì„¸ìš” ---
if __name__ == "__main__":
    try:
        print("="*50)
        print("ëª¨ë¸ ë¡œë” ì„œë¹„ìŠ¤ ë‹¨ë… ì‹¤í–‰ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        print("="*50)
        
        # ìœ„ì—ì„œ ìƒì„±ëœ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ì˜ load_model í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
        model_loader_service.load_model()
        
        print("\nğŸ‰ğŸ‰ğŸ‰ í…ŒìŠ¤íŠ¸ ì„±ê³µ! ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ğŸ‰ğŸ‰ğŸ‰")
        
    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë” ìì„¸í•œ ì •ë³´ë¥¼ ë³¼ ìˆ˜ ìˆë„ë¡ traceback ì¶”ê°€
        import traceback
        print(f"\nğŸ”¥ğŸ”¥ğŸ”¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨! ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e} ğŸ”¥ğŸ”¥ğŸ”¥")
        traceback.print_exc()