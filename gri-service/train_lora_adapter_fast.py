#!/usr/bin/env python3
# train_lora_adapter_fast.py - ìµœì í™”ëœ ë¹ ë¥¸ LoRA í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸

import sys
import os
import torch
import pandas as pd
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Union
from dataclasses import dataclass
import logging

# ë¡œì»¬ ëª¨ë“ˆ importë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import (
    LoraConfig, 
    get_peft_model, 
    prepare_model_for_kbit_training,
    TaskType
)
from datasets import Dataset

from app.foundation.config import MODEL_PATH, ADAPTERS_PATH, MODEL_ID
from train_knowledge import KnowledgeTrainer

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class FastLoRATrainingConfig:
    """ë¹ ë¥¸ LoRA í›ˆë ¨ ì„¤ì •"""
    # LoRA ì„¤ì • (ë” ì‘ì€ íŒŒë¼ë¯¸í„°ë¡œ ë¹ ë¥¸ í›ˆë ¨)
    lora_r: int = 8                     # LoRA rank (16â†’8ë¡œ ê°ì†Œ)
    lora_alpha: int = 16                # LoRA alpha (32â†’16ìœ¼ë¡œ ê°ì†Œ)
    lora_dropout: float = 0.1           # LoRA dropout
    target_modules: Optional[List[str]] = None    # íƒ€ê²Ÿ ëª¨ë“ˆ
    
    # í›ˆë ¨ ì„¤ì • (ì†ë„ ìµœì í™”)
    num_epochs: int = 1                 # ì—í¬í¬ ìˆ˜ (3â†’1ë¡œ ê°ì†Œ)
    learning_rate: float = 5e-4         # í•™ìŠµë¥  ì¦ê°€ (2e-4â†’5e-4)
    batch_size: int = 4                 # ë°°ì¹˜ í¬ê¸° ì¦ê°€ (2â†’4)
    gradient_accumulation_steps: int = 2 # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  (4â†’2ë¡œ ê°ì†Œ)
    max_length: int = 256               # ìµœëŒ€ ê¸¸ì´ ê°ì†Œ (512â†’256)
    
    # ì €ì¥ ì„¤ì •
    adapter_name: str = "gri_adapter_fast"
    save_steps: int = 50                # ì €ì¥ ê°„ê²© ê°ì†Œ
    eval_steps: int = 25                # í‰ê°€ ê°„ê²© ê°ì†Œ

class FastLoRATrainer:
    """ë¹ ë¥¸ LoRA ì–´ëŒ‘í„° í›ˆë ¨ í´ë˜ìŠ¤"""
    
    def __init__(self, config: FastLoRATrainingConfig):
        self.config = config
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = None
        self.tokenizer = None
        self.knowledge_trainer = KnowledgeTrainer()
        
        # ê¸°ë³¸ íƒ€ê²Ÿ ëª¨ë“ˆ ì„¤ì • (GPT-NeoX ì•„í‚¤í…ì²˜ìš©, í•µì‹¬ ëª¨ë“ˆë§Œ ì„ íƒ)
        if self.config.target_modules is None:
            self.config.target_modules = [
                "query_key_value",  # attentionì˜ Q, K, V í†µí•© ëª¨ë“ˆë§Œ
                "dense"            # attention output projectionë§Œ
                # MLP ëª¨ë“ˆ ì œì™¸ë¡œ í›ˆë ¨ ì†ë„ í–¥ìƒ
            ]
        
        logger.info(f"ë¹ ë¥¸ LoRA í›ˆë ¨ ì´ˆê¸°í™” - Device: {self.device}")

    def load_base_model(self):
        """ë² ì´ìŠ¤ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©"""
        logger.info(f"ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”©: {MODEL_ID}")
        
        # í† í¬ë‚˜ì´ì € ë¡œë”©
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # ëª¨ë¸ ë¡œë”© (ìµœì í™”ëœ ì„¤ì •)
        self.model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.float16,
            device_map=None,
            low_cpu_mem_usage=True,
            trust_remote_code=True,
        )
        
        # GPUë¡œ ëª…ì‹œì  ì´ë™
        if torch.cuda.is_available():
            self.model = self.model.to("cuda")
        
        logger.info("âœ… ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

    def prepare_fast_training_data(self) -> Dataset:
        """ë¹ ë¥¸ í›ˆë ¨ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„ (JSON í˜•íƒœë¡œ ìµœì í™”)"""
        logger.info("ë¹ ë¥¸ í›ˆë ¨ìš© ë°ì´í„° ì¤€ë¹„ ì¤‘...")
        
        try:
            # JSON í˜•íƒœì˜ ë¯¸ë¦¬ ì •ì˜ëœ í•µì‹¬ í›ˆë ¨ ë°ì´í„°
            fast_training_data = [
                {
                    "input": "ESGë€ ë¬´ì—‡ì¸ê°€ìš”?",
                    "output": "ESGëŠ” Environmental(í™˜ê²½), Social(ì‚¬íšŒ), Governance(ì§€ë°°êµ¬ì¡°)ì˜ ì¤„ì„ë§ë¡œ, ê¸°ì—…ì˜ ì§€ì†ê°€ëŠ¥ì„±ì„ í‰ê°€í•˜ëŠ” í•µì‹¬ ì§€í‘œì…ë‹ˆë‹¤."
                },
                {
                    "input": "GRI ìŠ¤íƒ ë‹¤ë“œì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                    "output": "GRI ìŠ¤íƒ ë‹¤ë“œëŠ” ì¡°ì§ì˜ ê²½ì œ, í™˜ê²½, ì‚¬íšŒì  ì˜í–¥ì„ ë³´ê³ í•˜ê¸° ìœ„í•œ êµ­ì œì  í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤."
                },
                {
                    "input": "ì§€ì†ê°€ëŠ¥ì„± ë³´ê³ ì„œì˜ ì¤‘ìš”ì„±ì€?",
                    "output": "ì§€ì†ê°€ëŠ¥ì„± ë³´ê³ ì„œëŠ” ê¸°ì—…ì˜ ESG ì„±ê³¼ë¥¼ íˆ¬ëª…í•˜ê²Œ ê³µê°œí•˜ì—¬ ì´í•´ê´€ê³„ì ì‹ ë¢°ë¥¼ êµ¬ì¶•í•˜ëŠ” ì¤‘ìš”í•œ ë„êµ¬ì…ë‹ˆë‹¤."
                }
            ]
            
            # GRI ë°ì´í„°ë„ ì¼ë¶€ ì¶”ê°€ (ì†ë„ë¥¼ ìœ„í•´ ì œí•œ)
            try:
                df = self.knowledge_trainer.load_gri_data()
                gri_count = 0
                for _, row in df.iterrows():
                    if gri_count >= 20:  # 20ê°œë§Œ ì„ íƒ
                        break
                        
                    try:
                        gongsi = str(row['ê³µì‹œì‚¬í•­']) if pd.notna(row['ê³µì‹œì‚¬í•­']) else ""  # type: ignore
                        yogusahang = str(row['ìš”êµ¬ì‚¬í•­']) if pd.notna(row['ìš”êµ¬ì‚¬í•­']) else ""  # type: ignore
                        
                        if gongsi and yogusahang and len(gongsi) < 100 and len(yogusahang) < 200:  # ê¸¸ì´ ì œí•œ
                            fast_training_data.append({
                                "input": f"{gongsi}ì— ëŒ€í•œ GRI ìš”êµ¬ì‚¬í•­ì„ ì•Œë ¤ì£¼ì„¸ìš”.",
                                "output": yogusahang[:200]  # ê¸¸ì´ ì œí•œ
                            })
                            gri_count += 1
                    except:
                        continue
            except Exception as e:
                logger.warning(f"GRI ë°ì´í„° ë¡œë”© ì‹¤íŒ¨, ê¸°ë³¸ ë°ì´í„°ë§Œ ì‚¬ìš©: {e}")
            
            # ëŒ€í™”í˜• í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜
            training_examples = []
            for item in fast_training_data:
                prompt = f"### ì§ˆë¬¸: {item['input']}\n\n### ë‹µë³€: {item['output']}"
                training_examples.append({"text": prompt})
            
            logger.info(f"ì´ {len(training_examples)}ê°œì˜ ë¹ ë¥¸ í›ˆë ¨ ì˜ˆì œ ìƒì„±")
            
            # Dataset ê°ì²´ ìƒì„±
            dataset = Dataset.from_list(training_examples)
            
            return dataset
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            raise

    def tokenize_function(self, examples):
        """ìµœì í™”ëœ í† í¬ë‚˜ì´ì§• í•¨ìˆ˜"""
        if self.tokenizer is None:
            raise ValueError("Tokenizerê°€ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        # í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§• (ìµœì í™”ëœ ì„¤ì •)
        tokenized = self.tokenizer(
            examples["text"],
            truncation=True,
            padding="max_length",  # ê³ ì • ê¸¸ì´ë¡œ íŒ¨ë”©
            max_length=self.config.max_length,
            return_tensors="pt"
        )
        
        # GPT-NeoXëŠ” token_type_idsë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì œê±°
        if "token_type_ids" in tokenized:
            del tokenized["token_type_ids"]
        
        # labelsë¥¼ input_idsì™€ ë™ì¼í•˜ê²Œ ì„¤ì •
        tokenized["labels"] = tokenized["input_ids"].clone()
        
        return tokenized

    def setup_fast_lora(self):
        """ìµœì í™”ëœ LoRA ì„¤ì •"""
        logger.info("ë¹ ë¥¸ LoRA ì„¤ì • ì¤‘...")
        
        if self.model is None:
            raise ValueError("ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ìµœì í™”ëœ LoRA ì„¤ì •
        lora_config = LoraConfig(
            r=self.config.lora_r,
            lora_alpha=self.config.lora_alpha,
            target_modules=self.config.target_modules,
            lora_dropout=self.config.lora_dropout,
            bias="none",
            task_type=TaskType.CAUSAL_LM,
        )
        
        # ëª¨ë¸ì— LoRA ì ìš©
        self.model = get_peft_model(self.model, lora_config)  # type: ignore
        
        # í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ì¶œë ¥
        self.model.print_trainable_parameters()
        
        logger.info("âœ… ë¹ ë¥¸ LoRA ì„¤ì • ì™„ë£Œ")

    def train(self):
        """ìµœì í™”ëœ ë¹ ë¥¸ í›ˆë ¨ ìˆ˜í–‰"""
        logger.info("ë¹ ë¥¸ LoRA í›ˆë ¨ ì‹œì‘...")
        
        try:
            # 1. ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”©
            self.load_base_model()
            
            # 2. ë¹ ë¥¸ LoRA ì„¤ì •
            self.setup_fast_lora()
            
            # 3. ë¹ ë¥¸ í›ˆë ¨ ë°ì´í„° ì¤€ë¹„
            dataset = self.prepare_fast_training_data()
            tokenized_dataset = dataset.map(
                self.tokenize_function,
                batched=True,
                remove_columns=dataset.column_names
            )
            
            # 4. ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
            output_dir = ADAPTERS_PATH / self.config.adapter_name
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # 5. ìµœì í™”ëœ í›ˆë ¨ ì¸ì ì„¤ì •
            training_args = TrainingArguments(
                output_dir=str(output_dir),
                num_train_epochs=self.config.num_epochs,
                per_device_train_batch_size=self.config.batch_size,
                gradient_accumulation_steps=self.config.gradient_accumulation_steps,
                learning_rate=self.config.learning_rate,
                logging_steps=5,                # ë¡œê¹… ê°„ê²© ë‹¨ì¶•
                save_steps=self.config.save_steps,
                eval_steps=self.config.eval_steps,
                warmup_steps=10,               # warmup ë‹¨ì¶• (100â†’10)
                lr_scheduler_type="linear",    # ë” ê°„ë‹¨í•œ ìŠ¤ì¼€ì¤„ëŸ¬
                fp16=True,                     # í˜¼í•© ì •ë°€ë„ í›ˆë ¨
                dataloader_num_workers=0,      # ë‹¨ìˆœí™”
                remove_unused_columns=False,
                report_to=None,
                save_total_limit=2,            # ì €ì¥ íŒŒì¼ ìˆ˜ ì œí•œ
            )
            
            # 6. ìµœì í™”ëœ ë°ì´í„° ì½œë ˆì´í„°
            if self.tokenizer is None:
                raise ValueError("Tokenizerê°€ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                
            data_collator = DataCollatorForLanguageModeling(
                tokenizer=self.tokenizer,
                mlm=False,
            )
            
            # 7. Trainer ìƒì„±
            trainer = Trainer(
                model=self.model,  # type: ignore
                args=training_args,
                train_dataset=tokenized_dataset,
                data_collator=data_collator,
            )
            
            # 8. ë¹ ë¥¸ í›ˆë ¨ ì‹¤í–‰
            logger.info("ğŸš€ ë¹ ë¥¸ í›ˆë ¨ ì‹œì‘!")
            start_time = datetime.now()
            
            train_result = trainer.train()
            
            end_time = datetime.now()
            training_time = end_time - start_time
            
            # 9. ì–´ëŒ‘í„° ì €ì¥
            trainer.save_model()
            
            # 10. í›ˆë ¨ ë¡œê·¸ ì €ì¥
            log_data = {
                "timestamp": datetime.now().isoformat(),
                "training_time_minutes": training_time.total_seconds() / 60,
                "config": self.config.__dict__,
                "train_result": train_result.metrics,
                "adapter_path": str(output_dir),
                "optimization": "fast_training"
            }
            
            log_file = output_dir / "fast_training_log.json"
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump(log_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… ë¹ ë¥¸ í›ˆë ¨ ì™„ë£Œ! ì†Œìš”ì‹œê°„: {training_time}")
            logger.info(f"ğŸ“ ì–´ëŒ‘í„° ì €ì¥ ìœ„ì¹˜: {output_dir}")
            logger.info(f"ğŸ“Š í›ˆë ¨ ë¡œê·¸: {log_file}")
            
            return output_dir
            
        except Exception as e:
            logger.error(f"âŒ ë¹ ë¥¸ í›ˆë ¨ ì‹¤íŒ¨: {e}")
            raise

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("âš¡ ë¹ ë¥¸ LoRA ì–´ëŒ‘í„° í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤!")
    print("="*60)
    
    # ìµœì í™”ëœ í›ˆë ¨ ì„¤ì •
    config = FastLoRATrainingConfig(
        adapter_name="gri_adapter_fast",
        num_epochs=1,        # ë¹ ë¥¸ í›ˆë ¨
        batch_size=4,        # ë°°ì¹˜ í¬ê¸° ì¦ê°€
        learning_rate=5e-4,  # í•™ìŠµë¥  ì¦ê°€
        max_length=256,      # ì‹œí€€ìŠ¤ ê¸¸ì´ ê°ì†Œ
    )
    
    # ì„¤ì • ì¶œë ¥
    print("ë¹ ë¥¸ í›ˆë ¨ ì„¤ì •:")
    print(f"  - LoRA Rank: {config.lora_r} (ê°ì†Œ)")
    print(f"  - LoRA Alpha: {config.lora_alpha} (ê°ì†Œ)")
    print(f"  - ì—í¬í¬: {config.num_epochs} (ê°ì†Œ)")
    print(f"  - ë°°ì¹˜ í¬ê¸°: {config.batch_size} (ì¦ê°€)")
    print(f"  - í•™ìŠµë¥ : {config.learning_rate} (ì¦ê°€)")
    print(f"  - ìµœëŒ€ ê¸¸ì´: {config.max_length} (ê°ì†Œ)")
    print(f"  - ì–´ëŒ‘í„° ì´ë¦„: {config.adapter_name}")
    print("="*60)
    
    # ì‚¬ìš©ì í™•ì¸
    response = input("ë¹ ë¥¸ í›ˆë ¨ì„ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
    if response not in ['y', 'yes']:
        print("í›ˆë ¨ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return
    
    try:
        # ë¹ ë¥¸ í›ˆë ¨ ì‹œì‘
        trainer = FastLoRATrainer(config)
        adapter_path = trainer.train()
        
        print("\nâš¡ ë¹ ë¥¸ í›ˆë ¨ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ğŸ“ ì–´ëŒ‘í„° ìœ„ì¹˜: {adapter_path}")
        print("\në‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì–´ëŒ‘í„°ë¥¼ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
        print("python tests/test_model_loader.py")
        
    except Exception as e:
        print(f"\nâŒ ë¹ ë¥¸ í›ˆë ¨ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 